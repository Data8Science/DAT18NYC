{
 "metadata": {
  "name": "",
  "signature": "sha256:f1f7dd51c802f6a2cb7b1aacab44743ae0ba468d37e750bcad73fe4d44afa080"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Matrix Algebra and the Regression Algorithm"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Today we'll review how matrix algebra solves for the ordinary least squares regression, different cost functions for different linear solutions, and how we can use a linear solution to solve for probability of a given class."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Objectives"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Understand how Matrix Algebra is used to find coefficients in an ordinary least squares regression\n",
      "* Review two techniques to improving the linear model\n",
      "* Understanding the use case and results of a Logistic Regression\n",
      "* New Cross Validation Technique - **K-Fold**\n",
      "* First Metric for Classification: **Accuracy**"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Code Dictionary"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Class Notes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Matrix Algebra Review\n",
      "\n",
      "In order to best understand most machine learning algorithms, we need some basis of linear algebra.\n",
      "\n",
      "> Linear algebra is best defined as mathematics in the multidimensional space and the mapping between said spaces.\n",
      "\n",
      "#### For Example\n",
      "\n",
      "$$y = mx + b$$<br />\n",
      "$$y = m_{1}x_{1} + m_{2}x_{2} + b$$<br />\n",
      "$$y = m_{1}x_{1} + m_{2}x_{2} + m_{3}x_{3} + m_{4}x_{4} + b$$<br />\n",
      "$$y = m_{1}x_{1} + m_{2}x_{2} + m_{3}x_{3} + m_{4}x_{4} + m_{5}x_{5} + m_{6}x_{6} + m_{7}x_{7} + m_{8}x_{8} + m_{9}x_{9} + m_{10}x_{10} + b$$\n",
      "\n",
      "### Matrices\n",
      "\n",
      "> Matrices are an array of real numbers with m rows and n columns\n",
      "\n",
      "Each value in a matrix is called an entry.\n",
      "\n",
      "$$\\begin{bmatrix}1 & 5 & 8 & 7\\\\2 & 1 & 3 & 6\\\\3 & 5 & 1 & 0\\\\4 & 6 & 0 & 1\\end{bmatrix}$$\n",
      "\n",
      "$A_{2,1}$ in the given matrix, refers to the entry on the 2nd row, in the 1st column. The value is 2. \n",
      "\n",
      "\n",
      "### Vectors\n",
      "\n",
      "> Vectors are a special kind of matrix, as they only consist of one dimension of real numbers.\n",
      "\n",
      "These look most like a numeric array (or **list**) in Python.\n",
      "\n",
      "$$\\begin{bmatrix}1 & 3 & 9 & 2\\end{bmatrix}$$\n",
      "\n",
      "Likewise, you can refer to each index or value similarly (`a[0]` in Python is the same entity as 0 in vector a)\n",
      "\n",
      "\n",
      "### Properties of Arrays\n",
      "\n",
      "**Rule 1!**\n",
      "\n",
      "Matrices can be added together only when they are the same size. If they are not the same size, their sum is **undefined**.\n",
      "\n",
      "$$\\begin{bmatrix}1 & 3 & 9 & 2\\end{bmatrix} + \\begin{bmatrix}2 & 5 & 9 & 4\\end{bmatrix} = \n",
      "\\begin{bmatrix}3 & 8 & 18 & 6\\end{bmatrix} $$\n",
      "\n",
      "$$\\begin{bmatrix}8 & 72 & 3 & 1\\end{bmatrix} + \\begin{bmatrix}17 & 55 & 3 & 10\\end{bmatrix} = \n",
      " \\ ? $$\n",
      " \n",
      " \n",
      "**Rule 2!**\n",
      "\n",
      "Matrices can be multiplied by a scalar (single entity) value.\n",
      "\n",
      "> Each value in the matrix is multiplied by the scalar value.\n",
      "\n",
      "$$\\begin{bmatrix}1 & 3 & 9 & 2\\end{bmatrix} * 3 =\n",
      "\\begin{bmatrix}3 & 9 & 27 & 6\\end{bmatrix} $$\n",
      "\n",
      "$$\\begin{bmatrix}8 & 72 & 3 & 1\\end{bmatrix} * 2 = \\ ? $$\n",
      "\n",
      "\n",
      "**Rule 3!**\n",
      "\n",
      "Matrices and vectors can be multiplied together given that the matrix columns are as wide as the vector is long. \n",
      "\n",
      "> The result will always be a vector.\n",
      "\n",
      "$$\\begin{bmatrix}1 & 3 & 9 & 2\\\\2 & 4 & 6 & 8\\end{bmatrix} * \\begin{bmatrix}2 \\\\ 3 \\\\ 6 \\\\ 5  \\end{bmatrix} = \\begin{matrix} 2 + 9 + 54 + 10 \\\\ 4 + 12 + 36 + 40 \\end{matrix} = \\begin{bmatrix} 75 \\\\ 92 \\end{bmatrix}$$\n",
      "\n",
      "**Rule 4!**\n",
      "\n",
      "Matrices can be multiplied together using the same rules that we have from matrix-vector multiplication.\n",
      "\n",
      "> The result will always be a matrix.\n",
      "\n",
      "$$\\begin{bmatrix}1 & 3 & 9 & 2\\\\2 & 4 & 6 & 8\\end{bmatrix} * \\begin{bmatrix}2 & 1 \\\\ 3 & 2 \\\\ 6 & 0 \\\\ 5 & 4 \\end{bmatrix} = \\begin{matrix} 2 + 9 + 54 + 10 \\\\ 4 + 12 + 36 + 40 \\\\ 1 + 6 + 0 + 8 \\\\ 2 + 8 + 0 + 32\\end{matrix} = \\begin{bmatrix} 75 & 15 \\\\ 92 & 42 \\end{bmatrix}$$\n",
      "\n",
      "\n",
      "### Why This Matters\n",
      "\n",
      "Matrices represent the multiple dimensions in our data! If we had a vector that suggested how important each dimension of our data was, we could use that to find our best **linear model**.\n",
      "\n",
      "\n",
      "### The Ordinary Least Squares (OLS) Linear Regression Formula\n",
      "\n",
      "A regression model is a functional relationship between input & response\n",
      "variables.\n",
      "\n",
      "The **simple linear regression** model captures a linear relationship between a single input variable x and a response variable y: \n",
      "\n",
      "$$y = \u03b1+\u03b2x+\u03b5$$\n",
      "\n",
      "* $y$ = response variable (the one we want to predict)\n",
      "* $x$ = input variable (the one we use to train the model)\n",
      "* $\u03b1$ = intercept (where the line crosses the y-axis)\n",
      "* $\u03b2$ = regression coefficient (the model \u201cparameter\u201d)\n",
      "* $\u03b5$ = residual (the prediction error)\n",
      "\n",
      "The **cost function** or goal of the ordinary least squares regression is to find the linear solution with the least sum of square error.\n",
      "\n",
      "#### Solving for OLS\n",
      "\n",
      "We'll break down the math here:\n",
      "\n",
      "The OLS Linear Regression is just matrix algebra (the stuff from up above!)\n",
      "\n",
      "Let\u2019s go over the math by hand so we can understand how we determine the regression coefficient.\n",
      "\n",
      "A linear regression in its simplest form:\n",
      "\n",
      "$y = \\alpha + \\beta x + \\epsilon$\n",
      "\n",
      "but we can assume that our $\\alpha$ (y-itercept)is either 0 or 1, and $\\epsilon$ (error) is zero.\n",
      "\n",
      "$y = \u03b2x$\n",
      "\n",
      "but we want to solve for \u03b2, which means our new equation now looks like this:\n",
      "\n",
      "$\u03b2 = ( X^TX)^{-1} X^Ty$\n",
      "\n",
      "\n",
      "#### How did we get there?\n",
      "\n",
      "The below is problematic, as we cannot divide by a matrix! So we first square the matrix.\n",
      "\n",
      "$\u03b2 = \\frac{y}x == \\frac{xy}{x^2}$\n",
      "\n",
      "Recall in algebra:\n",
      "\n",
      "$\\frac{1}x = x^{-1}$\n",
      "\n",
      "Inverting the matrix since raising $x$ to the power of negative 1 is equal to $1$ over $x$\n",
      "\n",
      "$\\frac{1}{x{^2}} * \\frac{xy}1$\n",
      "\n",
      "$(XX)^{-1}XY$\n",
      "\n",
      "And finally to make it programmer friendly:\n",
      "\n",
      "$\u03b2 = ( X^TX)^{-1} X^TY$\n",
      "\n",
      "#### Example\n",
      "\n",
      "So if we had data:\n",
      "\n",
      "    Input  Output\n",
      "    3.385  44.5\n",
      "    0.48   15.5\n",
      "    1.35   8.1\n",
      "    465    423\n",
      "    36.33  119.5\n",
      "\n",
      "$$\u03b2=\\left(\n",
      "    \\begin{array}{r}\n",
      "         \\begin{matrix}\n",
      "             1 & 1 & 1 & 1 & 1 & \\\\\n",
      "             3.385 & 0.48 & 1.35 & 465 & 36.33\n",
      "         \\end{matrix}\n",
      "         \\begin{matrix}\n",
      "            3.385 & 1 & \\\\\n",
      "            0.48 & 1 & \\\\\n",
      "            1.35 & 1 & \\\\\n",
      "            465 & 1 & \\\\\n",
      "            36.33 & 1\n",
      "         \\end{matrix}\n",
      "    \\end{array}\n",
      "  \\right)^{-1}\n",
      "  \\cdots\n",
      "$$\n",
      "  \n",
      "$$\u03b2 = ( X^TX)^{-1} \\cdots$$\n",
      "\n",
      "$$\\cdots\n",
      "    \\left(\n",
      "    \\begin{array}{r}\n",
      "         \\begin{matrix}\n",
      "             1 & 1 & 1 & 1 & 1 & \\\\\n",
      "             3.385 & 0.48 & 1.35 & 465 & 36.33\n",
      "         \\end{matrix}\n",
      "         \\begin{matrix}\n",
      "            44.5 & \\\\\n",
      "            15.5 & \\\\\n",
      "            8.1 & \\\\\n",
      "            423 & \\\\\n",
      "            119.5\n",
      "         \\end{matrix}\n",
      "    \\end{array}\n",
      "  \\right)\n",
      "$$\n",
      "  \n",
      "$$ \\cdots X^TY$$\n",
      "\n",
      "$$\u03b2=\\begin{array}{r}\n",
      "         \\begin{bmatrix}\n",
      "            0.2617 & -0.0006 & \\\\\n",
      "            -0.0006 & 0.000006\n",
      "         \\end{bmatrix}\n",
      "         \\begin{bmatrix}\n",
      "            610.6 & \\\\\n",
      "            201205.4425\n",
      "         \\end{bmatrix}\n",
      "    \\end{array}\n",
      "$$\n",
      "\n",
      "$$\u03b2 = ( X^TX)^{-1} X^TY$$\n",
      "\n",
      "$$ \\begin{bmatrix}\n",
      "            37.2 & \\\\\n",
      "            0.838\n",
      "         \\end{bmatrix}\n",
      "    = \\begin{array}{r}\n",
      "         \\begin{bmatrix}\n",
      "            0.2617 & -0.0006 & \\\\\n",
      "            -0.0006 & 0.000006\n",
      "         \\end{bmatrix}\n",
      "         \\begin{bmatrix}\n",
      "            610.6 & \\\\\n",
      "            201205.4425\n",
      "         \\end{bmatrix}\n",
      "    \\end{array}\n",
      "$$\n",
      "\n",
      "$$\u03b2 = ( X^TX)^{-1} X^TY$$\n",
      "\n",
      "$$ \\begin{array}{c}\n",
      "            Intercept : 37.2 & \\\\\n",
      "            \u03b2 : 0.838\n",
      "         \\end{array}\n",
      "$$\n",
      "\n",
      "#### Evaluate and verify in Python"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "from sklearn import linear_model as lm\n",
      "\n",
      "practice_set = pd.DataFrame({\n",
      "    'x': [3.385, 0.48, 1.35, 465, 36.33],\n",
      "    'y': [44.5, 15.5, 8.1, 423, 119.5],\n",
      "})\n",
      "\n",
      "model = lm.LinearRegression().fit(practice_set[['x']], practice_set['y'])\n",
      "#print model.intercept_\n",
      "#print model.coef_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Other cost techniques: Regularization\n",
      "\n",
      "Often times we do **not** want to solve with the OLS regression, because it may be accounting for *bias* in the data set, and not necessarily the *variance* within the data:\n",
      "\n",
      "* **Bias** refers to predictions that are systematically\n",
      "inaccurate. \n",
      "* **Variance** refers to predictions that are generally\n",
      "inaccurate.\n",
      "\n",
      "Therefore, it may make sense to adjust the cost function of our regression to trade fitting to the bias of the data set, so we may have a **less accurate model**, but a **better general, more applicable model**. Recall: our goal is not to find the best fit, but to find the fit that best explains the variance in our data.\n",
      "\n",
      "We define these with two **regularization** techniques:\n",
      "\n",
      "#### L1 regularization: Used when we have small data but many features.\n",
      "\n",
      "* minimize this: $ min(\\lVert y - x\u03b2 \\rVert^2 + \u03bb\\lVert x \\rVert)$\n",
      "* sklearn's math: $ min(\\lVert Xw - y \\rVert^2 + \\alpha\\lVert w \\rVert)$\n",
      "* solution: $y=\u03a3\u03b2_ix_i + \u03b5 \\quad st. \\quad \u03a3 \\lVert \u03b2_i \\rVert \\lt \u03bb$\n",
      "    \n",
      "#### L2 regularization: Used in just about all other cases.\n",
      "* minimize this: $ min(\\lVert y - x\u03b2 \\rVert^2 + \u03bb\\lVert x \\rVert^2)$\n",
      "* sklearn's math: $ min(\\lVert Xw - y \\rVert^2 + \\alpha\\lVert w \\rVert^2)$\n",
      "* solution: $y=\u03a3\u03b2_ix_i + \u03b5 \\quad st. \\quad \u03a3 \\lVert \u03b2_i^2 \\rVert \\lt \u03bb$\n",
      "\n",
      "\n",
      "#### What this looks like in our code: exploring other regressions in sklearn\n",
      "\n",
      "The linear_model module in sklearn is actually for regularized regressions. Each that include a regulization technique include a hyperparameter `alpha` to set--this would be the $\\lambda$ from above. A quick comparison:\n",
      "\n",
      "regression | class | Optimize L1? | Optimize L2?\n",
      "-----------|-------|---------------|--------------\n",
      "Ordinary Least Squares | `LinearRegression()` | - | -\n",
      "Ridge Regression | `Ridge()` | - | $\\checkmark$ \n",
      "Lasso Regression | `Lasso()` | $\\checkmark$ | - \n",
      "Elastic Net | `ElasticNet()` | $\\checkmark$  | $\\checkmark$ \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Try it: Fitting different models\n",
      "\n",
      "Start with the following base model, generated from a previous class:\n",
      "\n",
      "```python\n",
      "from sklearn import linear_model as lm\n",
      "from sklearn.datasets import load_boston\n",
      "\n",
      "boston = load_boston()\n",
      "desc = boston.DESCR\n",
      "bostondf = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
      "y_col = 'MEDV'\n",
      "bostondf[y_col] = boston.target\n",
      "x_cols = [\n",
      "    'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX',\n",
      "    'RM', 'AGE', 'DIS', 'RAD', 'TAX',\n",
      "    'PTRATIO', 'B', 'LSTAT'\n",
      "]\n",
      "\n",
      "model = lm.LinearRegression().fit(bostondf[x_cols], bostondf[y_col])\n",
      "```\n",
      "\n",
      "Small groups will work on only **one** of the following questions (do as assigned)\n",
      "\n",
      "1. Using one of the `Ridge()`, `Lasso()`, or `ElasticNet()` classes, write a for loop around that inserts a value for alpha, fits the model, and stores the Mean Square Error (Mean Square Error is in the metrics module of sklearn). Plot a line chart where x is the alpha parameter and y is the mean squared error. What does the line chart look like?\n",
      "2. Using each Regression class (all four from the above table), run a test train split, calculate the difference in $R^2$ between the test and train, and plot each result (x labels being the regression used, y being the difference in $R^2$ between test and train) Use the default alpha parameter. Which regression had the the most consistent result?\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Logistic Regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Logistic function\n",
      "\n",
      "Class probability from 0 to 1 represents a space of $0$ to $inf$. To use a linear model to determine probality of a given class, we want to use a function that gives us results from $-inf$ to $inf$.\n",
      "\n",
      "#### Demonstrate Failure of Linear Regression: Threshold Warping and cases > 1\n",
      "\n",
      "#### Advantages of the Logistic Regression\n",
      "* Interpretable (coefficients; weights)\n",
      "* Parameters are few : increase linearly with dimensionality\n",
      "* Extensible to multi-class\n",
      "* Foundation for Neural Networks and GLM\n",
      "\n",
      "#### Equation\n",
      "* $P(Y) = exp(\\alpha + \\beta x) / (1 + exp(\\alpha + \\beta x))$\n",
      "* a is location of the 50%, B is the rapidity of the rise\n",
      "\n",
      "#### Coefficient Estimation: Maximum Likelihood Estimation\n",
      "* Which values of the coefficients make the observed data most likely to have ocurred?\n",
      "* Take the Beta and raise $10^\\beta$ for odds ratio\n",
      "\n",
      "#### Odds, Log-odds\n",
      "\n",
      "The coefficients of the logistic regression represent the log-odds of the target given that feature. We'll need to use the `exp()` function to transform them into more human readable odds ratios.\n",
      "\n",
      "### Cross Validation: k-fold\n",
      "\n",
      "k-fold is splitting the dataset up evenly into a test and train split $k$ times. We want to determine:\n",
      "\n",
      "* do the cuts generally looks the same when generating a model?\n",
      "* should we consider the best model, or the average of k fits?\n",
      "\n",
      "### Metrics: Accuracy\n",
      "\n",
      "Accuracy measure number correctly predicted over total number of observations."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Review, Practice, Next Steps"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Mathematics\n",
      "[Great review on Linear Algebra](http://cs229.stanford.edu/section/cs229-linalg.pdf)\n",
      "\n",
      "#### Machine Learning\n",
      "[Early learning on its application to linear models](http://dept.stat.lsa.umich.edu/~kshedden/Courses/Stat401/Notes/401-multreg.pdf)\n",
      "\n",
      "#### Code\n",
      "[A Matrix Class implemented in pure python](http://code.activestate.com/recipes/189971-basic-linear-algebra-matrix/)\n",
      "\n",
      "* [Deck on Regularization](http://www.mit.edu/~9.520/spring07/Classes/rlsslides.pdf)\n",
      "* [Regression Analysis by Example](http://type.hk:2551/calibre/browse/book/294) (**Chapter 1 - 2**) - Samprit Chatterjee\n",
      "* [Matrix Algebra as a Tool](http://book.type.hk/calibre/browse/book/464) - Ali S. Hadi\n",
      "\n",
      "#### Logistic Regression\n",
      "* For more on logistic regression, watch the [first three videos](https://www.youtube.com/playlist?list=PL5-da3qGB5IC4vaDba5ClatUmFppXLAhE) (30 minutes total) from Chapter 4 of An Introduction to Statistical Learning.\n",
      "* UCLA's IDRE has a handy table to help you remember the [relationship between probability, odds, and log-odds](http://www.ats.ucla.edu/stat/mult_pkg/faq/general/odds_ratio.htm).\n",
      "* Better Explained has a very friendly introduction (with lots of examples) to the [intuition behind \"e\"](http://betterexplained.com/articles/an-intuitive-guide-to-exponential-functions-e/).\n",
      "* Here are some useful lecture notes on [interpreting logistic regression coefficients](http://www.unm.edu/~schrader/biostat/bio2/Spr06/lec11.pdf).\n",
      "* yhat has a great tutorial on logistic regression using [statsmodels](http://blog.yhathq.com/posts/logistic-regression-and-python.html)"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}