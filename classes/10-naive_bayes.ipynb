{
 "metadata": {
  "name": "",
  "signature": "sha256:31e6bf9eb69c68e08a908d965d4dc3d70a92c36c2d33ad16d3ab0942b6d69a66"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%html\n",
      "<link rel=\"stylesheet\" href=\"static/hyrule.css\" type=\"text/css\">"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<link rel=\"stylesheet\" href=\"static/hyrule.css\" type=\"text/css\">"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.HTML at 0x104396050>"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Naive Bayes Classification"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Objectives"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Understanding the similarities and differences between Bayes Classifiers and Logistic Regression\n",
      "* Using a vectorizer to transform your text data\n",
      "* Learning the difference between a sparse and normal array\n",
      "* Identifying and interpreting traits of learners through comparison"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "### A Classic Naive Bayes Example (80% of Doctors get this wrong):\n",
      "1% of women at age forty who participate in routine screening have breast cancer.  80% of women with breast cancer will get positive mammographies.  9.6% of women without breast cancer will also get positive mammographies.  A woman in this age group had a positive mammography in a routine screening.  \n",
      "\n",
      "What is the probability that she actually has breast cancer?\n",
      "\n",
      "> Take a moment to write out the answer\n",
      "\n",
      "<!--\n",
      "* Prior: 1% of women at age forty have breast cancer.\n",
      "* Posterior: Probability woman has breast cancer\n",
      "-->"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Class Notes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### What's the importance of the Naive Bayes Equation -- what do we need to know?\n",
      "\n",
      "#### Why Naive?\n",
      "It assumes all inputs (categories, predictors) are independent: In the case of text: its blind to word context and meaning: \n",
      "\n",
      "\n",
      "#### Naivete helps because negligible information is required\n",
      "Simply word counting and that's it -- faster testing and very simple construction on ANY text.  Naivete doesn't hurt because correctness is based on classification, not prediction.\n",
      "\n",
      "#### Good Predictor, (possibly) Bad Estimator\n",
      "NB chooses among possible categories to find the one that is the greatest.  The associated probability assigned is not necessarily accurate.  For example -- it might predict an email is Spam at 80%, but the true value might be 95% -- this doesn't matter, since its just choosing the category, anything over 50% will give it a perfect score. So while good estimation means good prediction,  good prediction doesn't necessarily mean good estimation.\n",
      "\n",
      "#### Bayes Theorem is Easily Derivable\n",
      "One simple way is through this Venn diagram via [Oscar Bonilla](http://oscarbonilla.com/2009/05/visualizing-bayes-theorem/).\n",
      "![](http://note.io/1nf77lD). \n",
      "\n",
      "These two steps, clear from the diagram above, can do it:\n",
      "      \n",
      "$P(AB) = \\dfrac{|AB|}{|U|}$\n",
      "\n",
      "$P(A|B) = \\dfrac{|AB|}{|B|}$\n",
      "\n",
      "       \n",
      "We then do some substitution...  \n",
      "\n",
      "$P(B|A) = \\dfrac{P(AB)P(B)}{P(A)}$\n",
      "\n",
      "$P(B|A) = \\dfrac{P(A|B)P(B)}{P(B)}$\n",
      "\n",
      "Which leaves us with a formula that will make more sense:\n",
      "\n",
      "$P(y|X_{n}) = \\dfrac{P(y)P(X_{n}|y)}{P(X_{n})}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### What's the difference between Multinomial and Bernoulli Naive Bayes?\n",
      "   * Try Bernoulli when your dictionary of words and document length is small.\n",
      "   * With Bernoulli, you may add in additional non-word features easily\n",
      "   * Multinomial usually performs better, takes into account frequency vs. presence\n",
      "   * Consider the probability for the prior of the term \"the\" -- Bernoulli: 100%; Multinomial 5%"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Multinomial vs. Bernoulli:\n",
      "![](http://note.io/1ftU7X6)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### A great explanation of Bayes Classification\n",
      "*from [Mathias Brandewinder](http://clear-lines.com/blog/post/Naive-Bayes-Classification.aspx)*\n",
      "\n",
      "The canonical application of Bayes na\u00efve classification is in text classification, where the goal is to identify to which pre-determined category a piece of text belongs to  \u2013 for instance, is this email I just received spam, or ham (\u201cvaluable\u201d email)?\n",
      "\n",
      "The underlying idea is to use individual words present in the text as indications for what category it is most likely to belong to, using Bayes Theorem, named after the cheerful-looking Reverend Bayes.\n",
      "\n",
      "Imagine that you received an email containing the words \u201cNigeria\u201d, \u201cPrince\u201d, \u201cDiamonds\u201d and \u201cMoney\u201d. It is very likely that if you look into your spam folder, you\u2019ll find quite a few emails containing these words, whereas, unless you are in the business of importing diamonds from Nigeria and have some aristocratic family, your \u201cnormal\u201d emails would rarely contain these words. They have a much higher frequency within the category \u201cSpam\u201d than within the Ham, which makes them a potential flag for undesired business ventures.\n",
      "\n",
      "On the other hand, let\u2019s assume that you are a lucky person, and that typically, what you receive is Ham, with the occasional Spam bit. If you took a random email in your inbox, it is then much more likely that it belongs to the Ham category.\n",
      "\n",
      "Bayes\u2019 Theorem combines these two pieces of information together, to determine the probability that a particular email belongs to the \u201cSpam\u201d category, if it contains the word \u201cNigeria\u201d:\n",
      "\n",
      "$P(is Spam|contains Nigeria) = \\dfrac{P(contains Nigeria|is Spam) P(is Spam)}{P(contains Nigeria)}$\n",
      "\n",
      "In other words, 2 factors should be taken into account when deciding whether an email containing \u201cNigeria\u201d is spam: how over-represented is that word in Spam, and how likely is it that any email is spammy in the first place?\n",
      "\n",
      "The algorithm is named \u201cNa\u00efve\u201d, because it makes a simplifying assumption about the text, which turns out to be very convenient for computations purposes, namely that each word appears with a frequency which doesn\u2019t depend on other words. This is an unlikely assumption (the word \u201cDiamond\u201d is much more likely to be present in an email containing \u201cNigeria\u201d than in your typical family-members discussion email).\n",
      "\n",
      "We\u2019ll leave it at that on the concepts \u2013  I\u2019ll refer the reader who want to dig deeper to the book, or to this explanation of text classification with Na\u00efve Bayes.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Data Problem: Predicting Moving Ratings Given Text Reviews"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn\n",
      "from sklearn import naive_bayes\n",
      "\n",
      "pd.set_option('display.width', 500)\n",
      "pd.set_option('display.max_columns', 30)\n",
      "\n",
      "critics = pd.read_csv('../data/rt_critics.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that Kaggle was running a [competition](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data) on accurately rating movies using this dataset.<br />\n",
      "Kaggle's another great location for random data problems, though they tend to be more focused on machine learning."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Vectorizing with SK-Learn"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "text = ['Math is great', 'Math is really great', 'Exciting exciting Math']\n",
      "\n",
      "CountVectorizer?\n",
      "\n",
      "# Like the learners in sklearn, CountVectorizer is a class; so `vectorizer` below represents an instance of that object.\n",
      "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
      "\n",
      "# call `fit` to build the vocabulary\n",
      "vectorizer.fit(text)\n",
      "\n",
      "# then, use `get_feature_names` to return the tokens\n",
      "print vectorizer.get_feature_names()\n",
      "\n",
      "# finally, call `transform` to convert text to a bag of words\n",
      "x = vectorizer.transform(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'exciting', u'exciting exciting', u'exciting math', u'great', u'is', u'is great', u'is really', u'math', u'math is', u'really', u'really great']\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Comparing a Sparse Matrix and Normal Matrix\n",
      "\n",
      "Vectorizers by default return back a sparse matrix. This is primarily because, as we'd expect, the majority of features will be 0, and at that point, it's more memory efficient to store the entry relationship and value. Pay attention to what we remember about matrix logic (how we refer to an entry) and how the toarray() function returns back the matrix we'd expect:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Sparse Matrix'\n",
      "print x # A compressed version; the \"sparse\" matrix.\n",
      "print type(x) # type is from scipy, a library we've spoken very little of, but is pretty fantastic.\n",
      "print\n",
      "print 'Matrix'\n",
      "x_back = x.toarray()\n",
      "print x_back"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Sparse Matrix\n",
        "  (0, 3)\t1\n",
        "  (0, 4)\t1\n",
        "  (0, 5)\t1\n",
        "  (0, 7)\t1\n",
        "  (0, 8)\t1\n",
        "  (1, 3)\t1\n",
        "  (1, 4)\t1\n",
        "  (1, 6)\t1\n",
        "  (1, 7)\t1\n",
        "  (1, 8)\t1\n",
        "  (1, 9)\t1\n",
        "  (1, 10)\t1\n",
        "  (2, 0)\t2\n",
        "  (2, 1)\t1\n",
        "  (2, 2)\t1\n",
        "  (2, 7)\t1\n",
        "<class 'scipy.sparse.csr.csr_matrix'>\n",
        "\n",
        "Matrix\n",
        "[[0 0 0 1 1 1 0 1 1 0 0]\n",
        " [0 0 0 1 1 0 1 1 1 1 1]\n",
        " [2 1 1 0 0 0 0 1 0 0 0]]\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we wanted the extra overhead, we could use the feature names as columns to a dataframe:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.DataFrame(x_back, columns=vectorizer.get_feature_names())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>exciting</th>\n",
        "      <th>exciting exciting</th>\n",
        "      <th>exciting math</th>\n",
        "      <th>great</th>\n",
        "      <th>is</th>\n",
        "      <th>is great</th>\n",
        "      <th>is really</th>\n",
        "      <th>math</th>\n",
        "      <th>math is</th>\n",
        "      <th>really</th>\n",
        "      <th>really great</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 2</td>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "   exciting  exciting exciting  exciting math  great  is  is great  is really  math  math is  really  really great\n",
        "0         0                  0              0      1   1         1          0     1        1       0             0\n",
        "1         0                  0              0      1   1         0          1     1        1       1             1\n",
        "2         2                  1              1      0   0         0          0     1        0       0             0"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Preparing our Features $X$ and Target $y$ for Training"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* $X$ is a `(nreview, nwords)` array. Each row corresponds to a bag-of-words representation for a single review. This will be the *input* to the model.\n",
      "* $y$ is a `nreview`-element 1/0 array, encoding whether a review is Fresh (1) or Rotten (0). This is the desired *output* "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print critics.quote[2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "A winning animated feature that has something for everyone on the age spectrum.\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create a vector where each row is bag-of-words for a single quote vectorizer is now going to forget the original feature set.\n",
      "What types of ngrams should we be getting given the settings we used when creating the vectorizer instance?\n",
      "\n",
      "What's the difference between fit and fit_transform?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rotten_vectorizer = vectorizer.fit(critics.quote)\n",
      "x = vectorizer.fit_transform(critics.quote)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create an array where each element encodes whether the array is Fresh or Rotten\n",
      "y = (critics.fresh == 'fresh').values.astype(int)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Creating the Classifier"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Basic Accuracy"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We've talked in class before about how accuracy can be a very simple measure to evaluate a classifier model performance. Below, we have a function that evaluates the accuracy of a test train split."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def train_and_measure(classifier, x, y):\n",
      "    from sklearn import cross_validation\n",
      "    \"\"\"\n",
      "    Function accepts a classifer from sklearn and computes the accuracy measure for a random train and test split\n",
      "    classifier: an sklearn class\n",
      "    x         : a matrix of features\n",
      "    y         : a vector of targets\n",
      "    \"\"\"\n",
      "    xtrain, xtest, ytrain, ytest = cross_validation.train_test_split(x, y, test_size=0.2, random_state=1234)\n",
      "    clf = classifier.fit(xtrain, ytrain)\n",
      "\n",
      "    #Print the accuracy on the test and training dataset\n",
      "    training_accuracy = clf.score(xtrain, ytrain)\n",
      "    test_accuracy = clf.score(xtest, ytest)\n",
      "    print classifier\n",
      "    print \"Accuracy on training data: %0.2f\" % (training_accuracy)\n",
      "    print \"Accuracy on test data:     %0.2f\" % (test_accuracy)\n",
      "    \n",
      "train_and_measure(naive_bayes.MultinomialNB(), x, y)\n",
      "\n",
      "x_ones = (x > 1) # recall that a bernoulli interpretation will only work with 1s and 0s, or binary data.\n",
      "train_and_measure(naive_bayes.BernoulliNB(), x_ones, y)\n",
      "\n",
      "# and for the heck of it:\n",
      "\n",
      "from sklearn import linear_model\n",
      "train_and_measure(linear_model.LogisticRegression(), x, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
        "Accuracy on training data: 0.99\n",
        "Accuracy on test data:     0.76\n",
        "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Accuracy on training data: 0.62\n",
        "Accuracy on test data:     0.59\n",
        "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Accuracy on training data: 1.00\n",
        "Accuracy on test data:     0.75\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(Look above\u2013it's our dear friend alpha!)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Your Turn: Writing a function that returns back the average and standard deviation of a k-fold cross validation\n",
      "\n",
      "We want a function that should do a very similar process as above, but now, to use k fold cross validation, and to return back two values: the mean and standard deviation accuracy from the k-fold train and test.\n",
      "\n",
      "Helper code below, use as you need. Steps to take:\n",
      "\n",
      "1. What are the arguments required in the function? What should be customizable?\n",
      "2. What code should never change?\n",
      "3. What should the return look like?\n",
      "\n",
      "After: Evaluate which model seems to do it's best on the set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import seaborn as sns\n",
      "from sklearn import cross_validation\n",
      "kfold = cross_validation.KFold(n=x_ones.shape[0], n_folds=5, shuffle=True, random_state=1234)\n",
      "\n",
      "train_acc = []\n",
      "test_acc = []\n",
      "for train_index, test_index in kfold:\n",
      "    clf = naive_bayes.MultinomialNB().fit(x[train_index], y[train_index])\n",
      "    train_acc.append(clf.score(x[train_index], y[train_index]))\n",
      "    test_acc.append(clf.score(x[test_index], y[test_index]))\n",
      "\n",
      "print np.array(test_acc).mean()\n",
      "print np.array(test_acc).std()\n",
      "\n",
      "plt.figure()\n",
      "sns.kdeplot(np.random.normal(loc=np.array(test_acc).mean(), scale=np.array(test_acc).std(), size=10000), shade=True)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.782121819222\n",
        "0.0102653931697\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 60,
       "text": [
        "<matplotlib.axes._subplots.AxesSubplot at 0x10dd4e410>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAECCAYAAAD0JMwBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10dd4e110>"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### How do we evaluate the right K to use in K fold?\n",
      "\n",
      "It's very common to default to a typical value of K, but let's evaluate, for a minute, what happens to the accuracy of our training and test sets as K increases:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "k_train_acc = []\n",
      "k_test_acc = []\n",
      "for i in range(2, 20):\n",
      "    kfold = cross_validation.KFold(n=x_ones.shape[0], n_folds=i, shuffle=True, random_state=1234)\n",
      "    test_acc, train_acc = [], []\n",
      "    for train_index, test_index in kfold:\n",
      "        clf = naive_bayes.MultinomialNB().fit(x[train_index], y[train_index])\n",
      "        train_acc.append(clf.score(x[train_index], y[train_index]))\n",
      "        test_acc.append(clf.score(x[test_index], y[test_index]))\n",
      "    k_train_acc.append(np.array(train_acc).mean())\n",
      "    k_test_acc.append(np.array(test_acc).mean())\n",
      "\n",
      "plt.figure()\n",
      "plt.plot(range(2, 20), k_train_acc)\n",
      "plt.plot(range(2, 20), k_test_acc)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 72,
       "text": [
        "[<matplotlib.lines.Line2D at 0x10cdcc910>]"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAECCAYAAAD9z2x7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nniwAAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10cf00e50>"
       ]
      }
     ],
     "prompt_number": 72
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The above we call a fit chart. How does this change as we attempt to use other classifiers? (Keep in mind when switching to LogisticRegression how much _slower_ it will be compared to Bayes!)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Another Metrics: The Confusion Matrix"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "wip"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Review errors\n",
      "bad_rotten = np.argsort(prob[Y == 0])[:5]\n",
      "bad_fresh = np.argsort(prob[Y == 1])[-5:]\n",
      "\n",
      "print \"Mis-predicted Rotten quotes\"\n",
      "print '---------------------------'\n",
      "for row in bad_rotten:\n",
      "    print critics[Y == 0].quote.irow(row)\n",
      "    print\n",
      "\n",
      "print \"Mis-predicted Fresh quotes\"\n",
      "print '--------------------------'\n",
      "for row in bad_fresh:\n",
      "    print critics[Y == 1].quote.irow(row)\n",
      "    print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Mis-predicted Rotten quotes\n",
        "---------------------------\n",
        "Has no one else found it highly peculiar that damn near everybody's choice for the best movie of (let's say) the decade should be dedicated, inferentially but absolutely, to the proposition that Courage is Madness and Cowardice is Best?"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "The performances are precise, the language is alive and well spoken and the setting is striking, but Vanya on 42nd Street still suffers rather heavily from the limitations of filmed theater.\n",
        "\n",
        "Lacking the surface glitz, attention-grabbing plot and star power of John Grisham's previous adaptations, Folley's film will also suffer due to comparisons with the similarly-themed and better Tim Robbins' Dead Man Walking, released last year.\n",
        "\n",
        "The fact that so much money was lavished on such a thick-headed project represents the height of fiscal ineptitude, but this is the kind of roller-coaster ride for which eager audiences will gladly check their brains at the turnstiles, so go figure.\n",
        "\n",
        "This is a film about deeply confused people that seems likely to put viewers in a state of deep confusion for most of its running time.\n",
        "\n",
        "Mis-predicted Fresh quotes\n",
        "--------------------------\n",
        "A good half-hour's worth of nonsense in the middle keeps Bad Boys from being little better than a break- even proposition.\n",
        "\n",
        "Though it's a good half hour too long, this overblown 1993 spin-off of the 60s TV show otherwise adds up to a pretty good suspense thriller.\n",
        "\n",
        "'Once Upon a Time...' now looks like an over-cooked mess of style, metaphor and reference.\n",
        "\n",
        "The movie's own payoff is compelling enough, but the project has a weightless feel that limits involvement. Better you give it an hour-and-a-half on video someday, surrounded by wine and snacks.\n",
        "\n",
        "There's too much talent and too strong a story to mess it up. There was potential for more here, but this incarnation is nothing to be ashamed of, and some of the actors answer the bell.\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 214
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "critics[critics['quote'].str.contains(\"audacious in its\")]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>critic</th>\n",
        "      <th>fresh</th>\n",
        "      <th>imdb</th>\n",
        "      <th>publication</th>\n",
        "      <th>quote</th>\n",
        "      <th>review_date</th>\n",
        "      <th>rtid</th>\n",
        "      <th>title</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>7658</th>\n",
        "      <td> Mick LaSalle</td>\n",
        "      <td> rotten</td>\n",
        "      <td> 386117</td>\n",
        "      <td> San Francisco Chronicle</td>\n",
        "      <td> Where the Wild Things Are is audacious in its ...</td>\n",
        "      <td> 2009-10-16</td>\n",
        "      <td> 770671948</td>\n",
        "      <td> Where the Wild Things Are</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 131,
       "text": [
        "            critic   fresh    imdb              publication                                              quote review_date       rtid                      title\n",
        "7658  Mick LaSalle  rotten  386117  San Francisco Chronicle  Where the Wild Things Are is audacious in its ...  2009-10-16  770671948  Where the Wild Things Are"
       ]
      }
     ],
     "prompt_number": 131
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "critics[critics['quote'].str.contains(\"own payoff is\")]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>critic</th>\n",
        "      <th>fresh</th>\n",
        "      <th>imdb</th>\n",
        "      <th>publication</th>\n",
        "      <th>quote</th>\n",
        "      <th>review_date</th>\n",
        "      <th>rtid</th>\n",
        "      <th>title</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>5965</th>\n",
        "      <td> Mike Clark</td>\n",
        "      <td> fresh</td>\n",
        "      <td> 115710</td>\n",
        "      <td> USA Today</td>\n",
        "      <td> The movie's own payoff is compelling enough, b...</td>\n",
        "      <td> 2000-01-01</td>\n",
        "      <td> 364525542</td>\n",
        "      <td> Blood and Wine</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 132,
       "text": [
        "          critic  fresh    imdb publication                                              quote review_date       rtid           title\n",
        "5965  Mike Clark  fresh  115710   USA Today  The movie's own payoff is compelling enough, b...  2000-01-01  364525542  Blood and Wine"
       ]
      }
     ],
     "prompt_number": 132
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Your Turn\n",
      "\n",
      "Given the text and classifier above, there is still **plenty** we can do to work through data cleanup and improve the model.\n",
      "\n",
      "1. Consider feature selection here: we haven't dropped a single feature! How could we evaluate this process? (check out `f_classif`)\n",
      "2. Last class we experiments with nltk and the part of speach tagger. Consider replacing words with parts of speach instead (using the default tagger), or other text processing tools (remove the stopwords... or just keep the stopwords!)\n",
      "3. What else is in the quote that we could identify and replace more easily?\n",
      "\n",
      "\n",
      "### More interest in sentiment?\n",
      "You can use the \"STS-Gold Sentiment Corpus\".  [CSV is here.](https://gist.githubusercontent.com/datadave/47ed59dd8733b2063dc6/raw/583615c70a1167fcd72899b2d2830493f1c616e6/sts_gold_tweet.csv)\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Review/Next Steps"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Naive Bayes\n",
      "* [SKL Naive Bayes Documentation](http://scikit-learn.org/stable/modules/naive_bayes.html)\n",
      "* [Stanford Naive Bayes Math](http://nlp.stanford.edu/IR-book/pdf/13bayes.pdf)"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}